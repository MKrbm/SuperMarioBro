{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Mario.ipynb","provenance":[{"file_id":"1rj50M2J4iYqMorbhdAvMaLnKTaUMf7sb","timestamp":1578140931418}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"PT6nqKQkzTQ7","colab_type":"code","colab":{}},"source":["!nvidia-smi"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"n7UXYrZA06xM","colab_type":"code","colab":{}},"source":["!apt-get install -y xvfb python-opengl > /dev/null 2>&1"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5ps0kYY-l3lp","colab_type":"code","colab":{}},"source":["!pip install gym pyvirtualdisplay > /dev/null 2>&1"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ts54IJPQsEYj","outputId":"f2b90374-2e05-4525-f7ee-54dbb1b3cdee","executionInfo":{"status":"ok","timestamp":1578381407828,"user_tz":-540,"elapsed":28972,"user":{"displayName":"keisuke murota","photoUrl":"","userId":"11993862372326833937"}},"colab":{"base_uri":"https://localhost:8080/","height":833}},"source":["# %%capture\n","%%bash\n","apt-get install cmake\n","apt-get install zlib1g-dev\n","pip install JSAnimation\n","pip install gym-super-mario-bros"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Reading package lists...\n","Building dependency tree...\n","Reading state information...\n","cmake is already the newest version (3.10.2-1ubuntu2.18.04.1).\n","The following package was automatically installed and is no longer required:\n","  libnvidia-common-430\n","Use 'apt autoremove' to remove it.\n","0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.\n","Reading package lists...\n","Building dependency tree...\n","Reading state information...\n","zlib1g-dev is already the newest version (1:1.2.11.dfsg-0ubuntu2).\n","zlib1g-dev set to manually installed.\n","The following package was automatically installed and is no longer required:\n","  libnvidia-common-430\n","Use 'apt autoremove' to remove it.\n","0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.\n","Collecting JSAnimation\n","  Downloading https://files.pythonhosted.org/packages/3c/e6/a93a578400c38a43af8b4271334ed2444b42d65580f1d6721c9fe32e9fd8/JSAnimation-0.1.tar.gz\n","Building wheels for collected packages: JSAnimation\n","  Building wheel for JSAnimation (setup.py): started\n","  Building wheel for JSAnimation (setup.py): finished with status 'done'\n","  Created wheel for JSAnimation: filename=JSAnimation-0.1-cp36-none-any.whl size=11425 sha256=b91ae56f7d5cd21610ce41e7f3b0335649c48b886e11e198a2f59667aa1397aa\n","  Stored in directory: /root/.cache/pip/wheels/3c/c2/b2/b444dffc3eed9c78139288d301c4009a42c0dd061d3b62cead\n","Successfully built JSAnimation\n","Installing collected packages: JSAnimation\n","Successfully installed JSAnimation-0.1\n","Collecting gym-super-mario-bros\n","  Downloading https://files.pythonhosted.org/packages/a0/b8/07460212c2568f78b02995834e7bdc25349e586473919e2983e01b984abf/gym_super_mario_bros-7.3.0-py2.py3-none-any.whl (198kB)\n","Collecting nes-py>=8.0.0\n","  Downloading https://files.pythonhosted.org/packages/00/98/f87eacc9ff3ddfe97ecc889165119317cd4782f5839c24b39f88a1a7e7d7/nes_py-8.1.1.tar.gz (74kB)\n","Requirement already satisfied: gym>=0.10.9 in /usr/local/lib/python3.6/dist-packages (from nes-py>=8.0.0->gym-super-mario-bros) (0.15.4)\n","Requirement already satisfied: numpy>=1.12.1 in /usr/local/lib/python3.6/dist-packages (from nes-py>=8.0.0->gym-super-mario-bros) (1.17.4)\n","Requirement already satisfied: pyglet>=1.3.2 in /usr/local/lib/python3.6/dist-packages (from nes-py>=8.0.0->gym-super-mario-bros) (1.3.2)\n","Requirement already satisfied: tqdm>=4.19.5 in /usr/local/lib/python3.6/dist-packages (from nes-py>=8.0.0->gym-super-mario-bros) (4.28.1)\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (from gym>=0.10.9->nes-py>=8.0.0->gym-super-mario-bros) (4.1.2.30)\n","Requirement already satisfied: cloudpickle~=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym>=0.10.9->nes-py>=8.0.0->gym-super-mario-bros) (1.2.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym>=0.10.9->nes-py>=8.0.0->gym-super-mario-bros) (1.3.3)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym>=0.10.9->nes-py>=8.0.0->gym-super-mario-bros) (1.12.0)\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet>=1.3.2->nes-py>=8.0.0->gym-super-mario-bros) (0.16.0)\n","Building wheels for collected packages: nes-py\n","  Building wheel for nes-py (setup.py): started\n","  Building wheel for nes-py (setup.py): finished with status 'done'\n","  Created wheel for nes-py: filename=nes_py-8.1.1-cp36-cp36m-linux_x86_64.whl size=447036 sha256=e3f178559840ffce0e2755c98f853f27cf606aa838a75ac5576e82544cfbb1dc\n","  Stored in directory: /root/.cache/pip/wheels/04/d7/e4/0949e4c8947993c5555730a3b15f3cdc5a86507b95388dd608\n","Successfully built nes-py\n","Installing collected packages: nes-py, gym-super-mario-bros\n","Successfully installed gym-super-mario-bros-7.3.0 nes-py-8.1.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"BhKa8jC-sQb-","colab":{}},"source":["import numpy as np\n","# import cPickle as pickle\n","import matplotlib.pyplot as plt\n","from JSAnimation.IPython_display import display_animation\n","import gym\n","from collections import deque\n","from matplotlib import animation, rc\n","from IPython.display import HTML\n","\n","%matplotlib inline"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"cUyBEmZYXvvV","colab_type":"code","outputId":"bd21e16b-e578-46fa-a090-41717360420c","executionInfo":{"status":"ok","timestamp":1578381408876,"user_tz":-540,"elapsed":29863,"user":{"displayName":"keisuke murota","photoUrl":"","userId":"11993862372326833937"}},"colab":{"base_uri":"https://localhost:8080/","height":71}},"source":["from pyvirtualdisplay import Display\n","display = Display(visible=0, size=(400, 300))\n","display.start()"],"execution_count":7,"outputs":[{"output_type":"stream","text":["xdpyinfo was not found, X start can not be checked! Please install xdpyinfo!\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["<Display cmd_param=['Xvfb', '-br', '-nolisten', 'tcp', '-screen', '0', '400x300x24', ':1001'] cmd=['Xvfb', '-br', '-nolisten', 'tcp', '-screen', '0', '400x300x24', ':1001'] oserror=None return_code=None stdout=\"None\" stderr=\"None\" timeout_happened=False>"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"3xKP6YMWulJE","colab":{}},"source":["from nes_py.wrappers import JoypadSpace\n","import gym_super_mario_bros\n","from gym_super_mario_bros.actions import SIMPLE_MOVEMENT\n","env = gym_super_mario_bros.make('SuperMarioBros-v0')\n","env = JoypadSpace(env, SIMPLE_MOVEMENT)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"a2ux1UZOlTPs","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":850},"outputId":"9adce951-27e9-48a6-e973-a76c9c38983c","executionInfo":{"status":"ok","timestamp":1578381410537,"user_tz":-540,"elapsed":31338,"user":{"displayName":"keisuke murota","photoUrl":"","userId":"11993862372326833937"}}},"source":["env = gym.make('CartPole-v1')\n","env.reset()\n","env.render(mode='rgb_array')"],"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[[255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        ...,\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255]],\n","\n","       [[255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        ...,\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255]],\n","\n","       [[255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        ...,\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255]],\n","\n","       ...,\n","\n","       [[255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        ...,\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255]],\n","\n","       [[255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        ...,\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255]],\n","\n","       [[255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        ...,\n","        [255, 255, 255],\n","        [255, 255, 255],\n","        [255, 255, 255]]], dtype=uint8)"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"VRyTL5ZzsWmx","colab":{}},"source":["def display_frames_as_gif(frames):\n","    \"\"\"\n","    Displays a list of frames as a gif, with controls\n","    \"\"\"\n","    #plt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi = 72)\n","    patch = plt.imshow(frames[0])\n","    plt.axis('off')\n","\n","    def animate(i):\n","        patch.set_data(frames[i])\n","\n","    anim = animation.FuncAnimation(plt.gcf(), animate, frames = len(frames), interval=100)\n","    return anim"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"aaRK7mLwJRGW","colab":{}},"source":["def preprocess(frame):\n","    frame = frame.sum(axis=-1)/765\n","    frame = frame[20:210,100:]\n","    frame = frame[::2,::2]\n","    return frame"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YmlvK7smUCIR","colab_type":"text"},"source":["## Models"]},{"cell_type":"code","metadata":{"id":"QWhY9hIg_w4x","colab_type":"code","colab":{}},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class QNetworkDuellingCNN1(nn.Module):\n","    def __init__(self, num_inputs, num_actions):\n","        super(ActorCritic, self).__init__()\n","        self.conv1 = nn.Conv2d(num_inputs, 32, 3, stride=2, padding=1)\n","        self.conv2 = nn.Conv2d(32, 32, 3, stride=2, padding=1)\n","        self.conv3 = nn.Conv2d(32, 32, 3, stride=2, padding=1)\n","        self.conv4 = nn.Conv2d(32, 32, 3, stride=2, padding=1)\n","        self.lstm = nn.LSTMCell(32 * 6 * 6, 512)\n","        self.critic_linear = nn.Linear(512, 1)\n","        self.actor_linear = nn.Linear(512, num_actions)\n","        self._initialize_weights()\n","\n","    def _initialize_weights(self):\n","        for module in self.modules():\n","            if isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear):\n","                nn.init.xavier_uniform_(module.weight)\n","                # nn.init.kaiming_uniform_(module.weight)\n","                nn.init.constant_(module.bias, 0)\n","            elif isinstance(module, nn.LSTMCell):\n","                nn.init.constant_(module.bias_ih, 0)\n","                nn.init.constant_(module.bias_hh, 0)\n","\n","    def forward(self, x, hx, cx):\n","        x = F.relu(self.conv1(x))\n","        x = F.relu(self.conv2(x))\n","        x = F.relu(self.conv3(x))\n","        x = F.relu(self.conv4(x))\n","        x = x.reshape(x.shape[0], -1)\n","        #hx, cx = self.lstm(x.view(x.size(0), -1), (hx, cx))\n","        return self.actor_linear(hx), self.critic_linear(hx), hx, cx\n","\n","\n","class QNetworkDuellingCNN(nn.Module):\n","    \"\"\"Actor (Policy) Model.\"\"\"\n","\n","    def __init__(self, channels, action_size, seed=42):\n","        \"\"\"Initialize parameters and build model.\n","        Params\n","        ======\n","            state_size (int): Dimension of each state\n","            action_size (int): Dimension of each action\n","            seed (int): Random seed\n","        \"\"\"\n","        super(QNetworkDuellingCNN, self).__init__()\n","        self.seed = torch.manual_seed(seed)\n","        self.conv1 = nn.Conv2d(channels, 4, 3, padding=1)\n","        self.conv2 = nn.Conv2d(4, 8, 3, padding=1)\n","        self.conv3 = nn.Conv2d(8, 16, 3, padding=1)\n","        self.conv4 = nn.Conv2d(16, 16, 3, padding=1)\n","        self.conv5 = nn.Conv2d(16, 16, 3, padding=1)\n","        self.bn1 = nn.BatchNorm2d(4)\n","        self.bn2 = nn.BatchNorm2d(8)\n","        self.bn3 = nn.BatchNorm2d(16)\n","        self.bn4 = nn.BatchNorm2d(16)\n","        self.bn5 = nn.BatchNorm2d(16)\n","        self.pool = nn.MaxPool2d(2, ceil_mode=True)\n","        \n","        self.dropout1 = nn.Dropout(0.2)\n","        self.dropout2 = nn.Dropout(0.5)\n","\n","        flat_len = 16*3*3\n","        self.fcval = nn.Linear(flat_len, 20)\n","        self.fcval2 = nn.Linear(20, 1)\n","        self.fcadv = nn.Linear(flat_len, 20)\n","        self.fcadv2 = nn.Linear(20, action_size)\n","\n","    def forward(self, x):\n","        \"\"\"Build a network that maps state -> action values.\"\"\"\n","        x = self.pool(\n","            F.relu(self.bn1(self.conv1(x))))\n","        \n","        x = self.pool(\n","            F.relu(self.bn2(self.conv2(x))))\n","        \n","        x = self.pool(\n","            F.relu(self.bn3(self.conv3(x))))\n","        \n","        x = self.pool(\n","            F.relu(self.bn4(self.conv4(x))))\n","        \n","        x = self.pool(\n","            F.relu(self.bn5(self.conv5(x))))\n","\n","        \n","        x = x.reshape(x.shape[0], -1)\n","        \n","        advantage = F.relu(self.fcadv(x))\n","        advantage = self.fcadv2(advantage)\n","        advantage = advantage - torch.mean(advantage, dim=-1, keepdim=True)\n","        \n","        value = F.relu(self.fcval(x))\n","        value = self.fcval2(value)\n","\n","        return value + advantage"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kizYu8ZVUWJQ","colab_type":"text"},"source":["## Memory buffer\n","Saves States, Actions, Rewards, Next States (SARS) and Dones.\n","\n","If priority sampling is required, we sample according to the error of the model."]},{"cell_type":"code","metadata":{"id":"vh4F0hv5-iHn","colab_type":"code","colab":{}},"source":["import torch\n","import numpy as np\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","class ReplayBuffer:\n","    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n","\n","    def __init__(self, state_size, action_size, buffer_size, batch_size, priority=False):\n","        \"\"\"Initialize a ReplayBuffer object.\n","        Params\n","        ======\n","            action_size (int): dimension of each action\n","            buffer_size (int): maximum size of buffer (chosen as multiple of num agents)\n","            batch_size (int): size of each training batch\n","            seed (int): random seed\n","        \"\"\"\n","        self.states = torch.zeros((buffer_size,)+state_size).to(device)\n","        self.next_states = torch.zeros((buffer_size,)+state_size).to(device)\n","        self.actions = torch.zeros(buffer_size,1, dtype=torch.long).to(device)\n","        self.rewards = torch.zeros(buffer_size, 1, dtype=torch.float).to(device)\n","        self.dones = torch.zeros(buffer_size, 1, dtype=torch.float).to(device)\n","        self.e = np.zeros((buffer_size, 1), dtype=np.float)\n","        \n","        self.priority = priority\n","\n","        self.ptr = 0\n","        self.n = 0\n","        self.buffer_size = buffer_size\n","        self.batch_size = batch_size\n","    \n","    def add(self, state, action, reward, next_state, done):\n","        \"\"\"Add a new experience to memory.\"\"\"\n","        self.states[self.ptr] = torch.from_numpy(state).to(device)\n","        self.next_states[self.ptr] = torch.from_numpy(next_state).to(device)\n","        self.actions[self.ptr] = action\n","        self.rewards[self.ptr] = reward\n","        self.dones[self.ptr] = done\n","        \n","        self.ptr += 1\n","        if self.ptr >= self.buffer_size:\n","            self.ptr = 0\n","            self.n = self.buffer_size\n","\n","    def sample(self, get_all=False):\n","        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n","        n = len(self)\n","        if get_all:\n","            return self.states[:n], self.actions[:n], self.rewards[:n], self.next_states[:n], self.dones[:n]\n","        # else:\n","        if self.priority:\n","            idx = np.random.choice(n, self.batch_size, replace=False, p=self.e)\n","        else:\n","            idx = np.random.choice(n, self.batch_size, replace=False)\n","        \n","        states = self.states[idx]\n","        next_states = self.next_states[idx]\n","        actions = self.actions[idx]\n","        rewards = self.rewards[idx]\n","        dones = self.dones[idx]\n","        \n","        return (states, actions, rewards, next_states, dones), idx\n","      \n","    def update_error(self, e, idx=None):\n","        e = torch.abs(e.detach())\n","        e = e / e.sum()\n","        if idx is not None:\n","            self.e[idx] = e.cpu().numpy()\n","        else:\n","            self.e[:len(self)] = e.cpu().numpy()\n","        \n","    def __len__(self):\n","        if self.n == 0:\n","            return self.ptr\n","        else:\n","            return self.n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6FVp2xSDU7bP","colab_type":"text"},"source":["## Agent\n","Actual agent and how to respond to the current state of the environment. Uses Models and Memory buffer from before.\n","\n","y_target = r + gamma * sum(future_rewards)\n","\n","sum(future_rewards) = q_local(state, action)"]},{"cell_type":"code","metadata":{"id":"eU94fRYi-iHq","colab_type":"code","colab":{}},"source":["import numpy as np\n","import random\n","from collections import namedtuple, deque\n","import itertools\n","\n","import torch\n","import torch.nn.functional as F\n","import torch.optim as optim\n","\n","# from ReplayBuffer import ReplayBuffer\n","\n","BUFFER_SIZE = int(5e3)  # replay buffer size\n","BATCH_SIZE = 256         # minibatch size\n","GAMMA = 0.99            # discount factor\n","TAU = 1e-3              # for soft update of target parameters\n","LR = 5e-4              # learning rate \n","UPDATE_EVERY = 10        # how often to update the network\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","            \n","\n","class DQNAgent():\n","    \"\"\"Interacts with and learns from the environment.\"\"\"\n","\n","    def __init__(self, model, state_size, action_size, seed=42, ddqn=False, priority=False):\n","        \"\"\"Initialize an Agent object.\n","        \n","        Params\n","        ======\n","            state_size (int): dimension of each state\n","            action_size (int): dimension of each action\n","            seed (int): random seed\n","        \"\"\"\n","        self.state_size = state_size\n","        self.action_size = action_size\n","        self.seed = random.seed(seed)\n","        self.ddqn = ddqn\n","\n","        # Q-Network\n","        self.qnetwork_local = model(state_size[0], action_size, seed).to(device)\n","        self.qnetwork_target = model(state_size[0], action_size, seed).to(device)\n","        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=LR)\n","\n","        # Replay memory\n","        self.memory = ReplayBuffer(state_size, (action_size,), BUFFER_SIZE, BATCH_SIZE)\n","        # Initialize time step (for updating every UPDATE_EVERY steps)\n","        self.t_step = 0\n","    \n","    def step(self, state, action, reward, next_state, done, sum_reward):\n","        # Save experience in replay memory\n","        self.memory.add(state, action, reward, next_state, done)\n","        self.sum_reward = sum_reward\n","        # Learn every UPDATE_EVERY time steps.\n","        self.t_step = (self.t_step + 1) % UPDATE_EVERY\n","        if self.t_step == 0:\n","            # If enough samples are available in memory, get random subset and learn\n","            if len(self.memory) > BATCH_SIZE:\n","                experiences, idx = self.memory.sample() # fetch all experiences correnponding to idx\n","                e = self.learn(experiences)\n","                self.memory.update_error(e, idx)\n","\n","    def act(self, state, eps=0.):\n","        \"\"\"Returns actions for given state as per current policy.\n","        \n","        Params\n","        ======\n","            state (array_like): current state\n","            eps (float): epsilon, for epsilon-greedy action selection\n","        \"\"\"\n","        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n","        self.qnetwork_local.eval() #\n","        with torch.no_grad():\n","            action_values = self.qnetwork_local(state)\n","        self.qnetwork_local.train()\n","\n","\n","        # Epsilon-greedy action selection\n","        if random.random() > eps:\n","            return np.argmax(action_values.cpu().data.numpy())\n","        else:\n","            return random.choice(np.arange(self.action_size))\n","          \n","    def update_error(self):\n","        states, actions, rewards, next_states, dones = self.memory.sample(get_all=True)\n","        with torch.no_grad():\n","            if self.ddqn:\n","                old_val = self.qnetwork_local(states).gather(-1, actions)\n","                actions = self.qnetwork_local(next_states).argmax(-1, keepdim=True)\n","                maxQ = self.qnetwork_target(next_states).gather(-1, actions)\n","                target = rewards+GAMMA*maxQ*(1-dones)\n","            else: # Normal DQN\n","                maxQ = self.qnetwork_target(next_states).max(-1, keepdim=True)[0]\n","                target = rewards+GAMMA*maxQ*(1-dones)\n","                old_val = self.qnetwork_local(states).gather(-1, actions)\n","            e = old_val - target\n","            self.memory.update_error(e)\n","\n","    def learn(self, experiences):\n","        \"\"\"Update value parameters using given batch of experience tuples.\n","        Params\n","        ======\n","            experiences (Tuple[torch.Variable]): tuple of (s, a, r, s', done) tuples \n","            gamma (float): discount factor\n","        \"\"\"\n","        states, actions, rewards, next_states, dones = experiences\n","\n","        ## compute and minimize the loss\n","        self.optimizer.zero_grad()\n","        if self.ddqn:\n","            old_val = self.qnetwork_local(states).gather(-1, actions) #Q(s,a,old_theta)\n","            with torch.no_grad():\n","                next_actions = self.qnetwork_local(next_states).argmax(-1, keepdim=True)\n","                maxQ = self.qnetwork_target(next_states).gather(-1, next_actions)\n","                target = rewards+GAMMA*maxQ*(1-dones)\n","        else: # Normal DQN\n","            with torch.no_grad():\n","                maxQ = self.qnetwork_target(next_states).max(-1, keepdim=True)[0]\n","                target = rewards+GAMMA*maxQ*(1-dones)\n","            old_val = self.qnetwork_local(states).gather(-1, actions)   \n","        \n","        loss = F.mse_loss(old_val, target) #mean square error\n","        loss.backward()\n","        self.optimizer.step()\n","\n","        # ------------------- update target network ------------------- #\n","        self.soft_update(self.qnetwork_local, self.qnetwork_target, TAU) \n","        \n","        return old_val - target\n","\n","\n","    def soft_update(self, local_model, target_model, tau):\n","        \"\"\"Soft update model parameters.\n","        θ_target = τ*θ_local + (1 - τ)*θ_target\n","        Params\n","        ======\n","            local_model (PyTorch model): weights will be copied from\n","            target_model (PyTorch model): weights will be copied to\n","            tau (float): interpolation parameter \n","        \"\"\"\n","        #self.sum_reward = 400/self.sum_reward\n","        #tau *= self.sum_reward\n","        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n","            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bc8_rpzfLKLD","colab_type":"text"},"source":["## Edit Reward system\n"]},{"cell_type":"code","metadata":{"id":"FyMwjvrzLFgm","colab_type":"code","colab":{}},"source":["class make_wrapper(gym.Wrapper):\n","    def __init__(self, env=None):\n","        \"\"\"Return only every `skip`-th frame\"\"\"\n","        super(make_wrapper, self).__init__(env)\n","        self.x = 40\n","        self.x_old = 40\n","        self.time = 400\n","        self.time_old = 400\n","        self.first = True\n","        self.score = 0\n","        self.score_old = 0\n","        self.get_over = False\n","\n","    def step(self, action):\n","        # obs, total_reward, done, info = self.env.step(action) #Using the same action during for loop.\n","        # self.x = info[\"x_pos\"]\n","        # self.time = info[\"time\"]\n","        # self.score = info[\"score\"]\n","        #total_reward = 0\n","        xs = deque(maxlen=5)\n","        rw = deque(maxlen=5)\n","        for _ in range(5):\n","            obs, reward, done, info = self.env.step(action)\n","            xs.append(info[\"x_pos\"])\n","            rw.append(reward)\n","            #total_reward += reward\n","            if done:\n","                obs = self.env.reset()\n","                break\n","        \n","        self.x = info[\"x_pos\"]\n","        self.time = info[\"time\"]\n","        self.score = info[\"score\"]\n","\n","        neg=any(ele < -10 for ele in rw)\n","        pos=any(ele > 10 for ele in rw)\n","\n","        if not neg and not pos:\n","            total_reward = (np.array(xs)[-1]-np.array(xs)[0])*0.5\n","        elif neg:\n","            total_reward = -20\n","\n","        elif pos:\n","            total_reward = 20\n","\n"," #       if self.x > self.x_old:\n","          #  self.x_old = self.x\n","\n","        # elif self.x_old >= self.x:\n","        #     if self.first:\n","        #         self.time_old = self.time\n","        #         self.first = False\n","        #     if self.time_old > self.time + 3:\n","        #         total_reward -= 1\n","        #         self.get_over = True\n","        # elif self.x_old <= self.x and not self.first and self.get_over:\n","        #     self.first = True\n","        #     self.time_old = self.time\n","        #     total_reward += 10\n","        #     print(\"get over!!\")\n","\n","        \n","        if self.score == self.score_old + 100:\n","            total_reward += 10\n","            self.score_old = self.score\n","\n","\n","        return obs, total_reward, done, info\n","  \n","\n","# class action_make(gym.AcitionWrapper):\n","#     def __init__(self, env):\n","#         super().__init__(env)\n","#         self.action_space = gym.spaces.Discrete(8)\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jhhfln09SeE2","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"52ecae9e-91e4-410e-baeb-f218f8f8b36d","executionInfo":{"status":"ok","timestamp":1578381703685,"user_tz":-540,"elapsed":1043,"user":{"displayName":"keisuke murota","photoUrl":"","userId":"11993862372326833937"}}},"source":["env = gym_super_mario_bros.make('SuperMarioBros-v0')\n","env = JoypadSpace(env, SIMPLE_MOVEMENT)\n","#env = make_wrapper(env)\n","print(env.action_space)\n","action=gym.spaces.Discrete(8)\n","action.sample()\n","env.get_action_meanings()"],"execution_count":30,"outputs":[{"output_type":"stream","text":["Discrete(7)\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["['NOOP', 'right', 'right A', 'right B', 'right A B', 'A', 'left']"]},"metadata":{"tags":[]},"execution_count":30}]},{"cell_type":"markdown","metadata":{"id":"_KGFLG3OVNuo","colab_type":"text"},"source":["## Train Agent\n","We step through different iterations of the environment to learn the optimal action for a given state."]},{"cell_type":"code","metadata":{"id":"2mc-TsIG-iHt","colab_type":"code","colab":{}},"source":["episode = 100\n","discount_rate = .99\n","noise = 0.1\n","noise_decay = 0.99\n","tmax = 400\n","target_reward = 1500\n","\n","# keep track of progress\n","sum_rewards = [0]\n","\n","\n","# keep track of frames\n","FRAME_SHAPE = (95, 78) #(210-20)/2 , 256/2\n","MAX_FRAMES = 5\n","nn_frames = deque(maxlen=MAX_FRAMES)\n","xs = deque(maxlen = 20)\n","for i in range(MAX_FRAMES):\n","    nn_frames.append(np.zeros(FRAME_SHAPE))\n","    \n","action_size = 7 #len(valid_actions)\n","state_size = (MAX_FRAMES,) + FRAME_SHAPE\n","agent = DQNAgent(QNetworkDuellingCNN, state_size, action_size, ddqn=True, priority=True)\n","e = 0\n","j = 0\n","data = []\n","ave = 0\n","while ave < 1100: #np.mean(sum_rewards[-5:]) < target_reward:\n","    obs = env.reset()\n","    prev_obs = None\n","    sum_reward = 0\n","    max_x = 0\n","    new_x = 0\n","    for i in range(MAX_FRAMES):\n","        nn_frames.append(np.zeros(FRAME_SHAPE)) # elements of the deque are all zero\n","    nn_frames.append(np.copy(preprocess(obs)))\n","    states = np.array(nn_frames)\n","    #print(np.shape(np.array(nn_frames)))\n","    for t in range(tmax):\n","    #while True:\n","        actions = agent.act(states, noise)\n","        obs, reward, done, info = env.step(actions)\n","        nn_frames.append(np.copy(preprocess(obs)))\n","        next_states = np.array(nn_frames)\n","        new_x = info[\"x_pos\"]\n","        if new_x > max_x:\n","            max_x = new_x\n","        \n","        agent.step(states, int(actions), int(reward), next_states, int(done), int(sum_rewards[-1]))\n","        sum_reward += reward\n","        states = next_states\n","        if done or reward < -10: # if mario die or game clear\n","            break\n","    \n","    agent.update_error()\n","    sum_rewards.append(sum_reward)\n","    e += 1\n","    print('\\rEpisode {}\\tCurrent Score: {:.2f} \\t max_x : {}'.format(e, sum_rewards[-1], max_x), end=\"\")\n","    # display some progress every 20 iterations\n","    xs.append(max_x)\n","    noise *= noise_decay\n","    if (e+1) % (10) ==0:\n","        obs = env.reset()\n","        prev_obs = None\n","        Frames = []\n","\n","        for i in range(MAX_FRAMES):\n","          nn_frames.append(np.zeros(FRAME_SHAPE))\n","        nn_frames.append(np.copy(preprocess(obs)))\n","        states = np.array(nn_frames)\n","        for t in range(500):\n","            Frames.append(np.array(obs,dtype=np.uint8))\n","            actions = agent.act(states, noise)\n","            obs, reward, done, info = env.step(actions)\n","            new_x = info[\"x_pos\"]\n","            if new_x > max_x:\n","                max_x = new_x\n","            nn_frames.append(np.copy(preprocess(obs)))\n","            next_states = np.array(nn_frames)\n","            sum_reward += reward\n","            states = next_states     \n","            if done:\n","                break\n","        anim=display_frames_as_gif(np.array(Frames))\n","        data.append(anim)\n","\n","        j += 1\n","        #tmax = int(np.array(xs).mean()*0.5)\n","        ave = np.array(xs).mean()\n","        print(\" | Episode: {0:d}, average score: {1:f}, max_x_ave : {2:f}\".format(e+1,np.mean(sum_rewards[-20:]),np.array(xs).mean()))\n","        "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"pkdHegxXOkUw","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QsYH73aq-iH3","colab_type":"code","colab":{}},"source":["obs = env.reset()\n","prev_obs = None\n","sum_reward = 0\n","\n","frames = []#np.zeros((2000, 240, 256, 3), dtype=np.uint8)\n","\n","for i in range(MAX_FRAMES):\n","  nn_frames.append(np.zeros(FRAME_SHAPE))\n","nn_frames.append(np.copy(preprocess(obs)))\n","states = np.array(nn_frames)\n","rs = []\n","xs = []\n","ys = []\n","i=0\n","#for t in range(500):\n","while True:\n","    i += 1\n","    actions = agent.act(states, 0.05)\n","    obs, reward, done, info = env.step(actions)\n","    frames.append(np.array(obs,dtype=np.uint8))\n","    nn_frames.append(np.copy(preprocess(obs)))\n","    next_states = np.array(nn_frames)\n","\n","    sum_reward += reward\n","    states = next_states\n","    rs.append(reward)\n","    xs.append(info['x_pos'])\n","    ys.append(info['y_pos'])\n","    if max(xs) > 1400:\n","    \n","      if reward < -10:\n","          print(\"i=\",i)\n","          break\n","\n","print('Sum of rewards is ', sum(rs))\n","plt.plot(rs)\n","plt.show()\n","\n","plt.plot(xs)\n","plt.show()\n","\n","anim=display_frames_as_gif(np.array(frames))\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ewYFgexmT96f","colab_type":"code","colab":{}},"source":["HTML(data[2].to_html5_video())\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"oFDTDvoNcEDU","colab_type":"code","colab":{}},"source":["import time\n","obs = env.reset()\n","plt.figure()\n","y_max=0\n","for _ in range(100):\n","  clear_output(wait=True)\n","  if _ <= 5:\n","    obs,_,_,info = env.step(2)\n","  else:\n","    obs,_,_,_ = env.step(0)\n","  # plt.imshow(obs)\n","  # plt.show()\n","  # time.sleep(0.1)\n","  y_new=info[\"y_pos\"]\n","  if y_new > y_max:\n","    y_max = y_new\n","print(y_max)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Vh-zdfxycbj0","colab_type":"code","colab":{}},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from IPython.display import clear_output\n","import time\n","\n","n_particles = 100\n","\n","for x in range(50):\n","  clear_output(wait=True)\n","  positions_x = [np.random.random() for i in range(n_particles)]\n","  positions_y = [2*(np.random.random()-0.5) for i in range(n_particles)]\n","\n","  plt.plot(positions_x , positions_y,'ro')\n","  plt.axis((-1.0 , 1.0 , -1.0 , 1.0))\n","  plt.show() \n","  time.sleep(0.01)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"woyEkYGyeG6B","colab_type":"code","colab":{}},"source":["a = np.random.rand(3,3)\n","b = np.random.rand(3,3)\n","l = [a,b]\n","np.shape(np.array(l))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Xv6ptnq9zH69","colab_type":"code","colab":{}},"source":["!nvidia-smi"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"CT1aMKex-iHx","colab_type":"code","colab":{}},"source":["plt.plot(sum_rewards)\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"xkRbsYRw3IHP","colab_type":"code","colab":{}},"source":["# print(np.shape(rs),i)\n","# print(done)\n","# li = []\n","# li.append(a)\n","# li.append(b)\n","# print(np.shape(np.array(frames)))\n","# plt.imshow(frames[50])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZZTJsONqQD_8","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"YobFvUL1pITi","outputId":"b1b9240a-c9fb-4844-a956-b1a665624e62","executionInfo":{"status":"ok","timestamp":1578381783183,"user_tz":-540,"elapsed":22569,"user":{"displayName":"keisuke murota","photoUrl":"","userId":"11993862372326833937"}},"colab":{"base_uri":"https://localhost:8080/","height":139}},"source":["# Mount Google Drive\n","from google.colab import drive # import drive from google colab\n","\n","ROOT = \"/content/drive\"     # default location for the drive\n","print(ROOT)                 # print content of ROOT (Optional)\n","\n","drive.mount(ROOT)           # we mount the google drive at /content/drive"],"execution_count":31,"outputs":[{"output_type":"stream","text":["/content/drive\n","Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","outputId":"44a1afdf-eadd-4e08-cf14-0fac579fd445","executionInfo":{"status":"ok","timestamp":1578381784776,"user_tz":-540,"elapsed":23845,"user":{"displayName":"keisuke murota","photoUrl":"","userId":"11993862372326833937"}},"id":"166fMASfpITp","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["!pwd"],"execution_count":32,"outputs":[{"output_type":"stream","text":["/content\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","outputId":"2d3b2afb-3677-46f7-a8b7-7904d60f60fa","executionInfo":{"status":"ok","timestamp":1578381784777,"user_tz":-540,"elapsed":23546,"user":{"displayName":"keisuke murota","photoUrl":"","userId":"11993862372326833937"}},"id":"kAaos4QPpITt","colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["# Clone github repository setup\n","# import join used to join ROOT path and MY_GOOGLE_DRIVE_PATH\n","from os.path import join  \n","\n","# path to your project on Google Drive\n","MY_GOOGLE_DRIVE_PATH = \"drive/My Drive/supermario/\"\n","# replace with your Github username \n","GIT_USERNAME = \"keisukemurota\" \n","# definitely replace with your\n","GIT_TOKEN = \"4506936c4f39c6b8a6bbdf9f2e8a4949bc9737c6\"  \n","# Replace with your github repository in this case we want \n","# to clone deep-learning-v2-pytorch repository\n","GIT_REPOSITORY = \"SuperMarioBro\" \n","\n","PROJECT_PATH = join(ROOT, MY_GOOGLE_DRIVE_PATH)\n","\n","# It's good to print out the value if you are not sure \n","print(\"PROJECT_PATH: \", PROJECT_PATH)   \n","\n","# In case we haven't created the folder already; we will create a folder in the project path \n","#!mkdir \"{PROJECT_PATH}\"    \n","\n","#GIT_PATH = \"https://{GIT_TOKEN}@github.com/{GIT_USERNAME}/{GIT_REPOSITORY}.git\" this return 400 Bad Request for me\n","GIT_PATH = \"https://\" + GIT_TOKEN + \"@github.com/\" + GIT_USERNAME + \"/\" + GIT_REPOSITORY + \".git\"\n","print(\"GIT_PATH: \", GIT_PATH)"],"execution_count":33,"outputs":[{"output_type":"stream","text":["PROJECT_PATH:  /content/drive/drive/My Drive/supermario/\n","GIT_PATH:  https://4506936c4f39c6b8a6bbdf9f2e8a4949bc9737c6@github.com/keisukemurota/SuperMarioBro.git\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","outputId":"e8c71772-30fc-4707-e8a4-35410f21a467","executionInfo":{"status":"ok","timestamp":1578381797833,"user_tz":-540,"elapsed":477,"user":{"displayName":"keisuke murota","photoUrl":"","userId":"11993862372326833937"}},"id":"McTdae-WpITv","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["path = \"drive/My Drive/SuperMarioBro\"\n","%cd \"{path}\""],"execution_count":35,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/SuperMarioBro\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","outputId":"55aae939-3d63-4f40-959d-8aa37a052597","executionInfo":{"status":"ok","timestamp":1578381891292,"user_tz":-540,"elapsed":2398,"user":{"displayName":"keisuke murota","photoUrl":"","userId":"11993862372326833937"}},"id":"DUNO5oci4-RX","colab":{"base_uri":"https://localhost:8080/","height":238}},"source":["!git add .\n","!git commit -m \"modified step and reward (make step)\""],"execution_count":39,"outputs":[{"output_type":"stream","text":["On branch branch\n","Changes to be committed:\n","  (use \"git reset HEAD <file>...\" to unstage)\n","\n","\t\u001b[32mmodified:   Mario.ipynb\u001b[m\n","\t\u001b[32mnew file:   Policy Gradient with Cartpole and PyTorch (Medium Version).ipynb\u001b[m\n","\n","Changes not staged for commit:\n","  (use \"git add <file>...\" to update what will be committed)\n","  (use \"git checkout -- <file>...\" to discard changes in working directory)\n","\n","\t\u001b[31mmodified:   Mario.ipynb\u001b[m\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"FmHDe1Ri4vhg","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"um0jOIy79f6h","colab_type":"code","colab":{}},"source":["!git push origin branch"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"9HtlLLOppITz","colab":{}},"source":["!git config --global user.email \"mukeisuke0709@gmail.com\"\n","!git config --global user.name \"keisukemurota\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"FW_iUUNnJBkn","colab_type":"code","colab":{}},"source":["!git remote -v"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"gHEL8E8GJFfu","colab_type":"code","colab":{}},"source":["! git status"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9fRDPSYvJIW5","colab_type":"code","colab":{}},"source":["! ls"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"29Pj_AFHJNKO","colab_type":"code","colab":{}},"source":["!git clone https://github.com/davidchiou/DDQN_Mario.git"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-mKFNzY0qsld","colab_type":"code","colab":{}},"source":["!python main.py"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"HAbMm5Lxq7YP","colab_type":"code","colab":{}},"source":["T = torch.tensor([100,0.5,10])\n","T = Categorical(T)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nCWS1aMLhzeX","colab_type":"code","colab":{}},"source":["from torch.distributions import Categorical"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9TnFUbrxh4RR","colab_type":"code","colab":{}},"source":["\n","T.sample()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"L2S8ZyOejM37","colab_type":"code","colab":{}},"source":["help(Categorical)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qk2hlM0JjfRw","colab_type":"code","colab":{}},"source":["from fastai.utils.show_install import *\n","show_install()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_5WTxTyuF3wS","colab_type":"code","colab":{}},"source":["4/vAHrka0LSvdRfWgXuiL4P80U6IlaTuQjZWCZ1TGB1eM1ZiuyLzZB4wc"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Mgc9nHIqGSsC","colab_type":"code","colab":{}},"source":["export IMAGE_FAMILY=\"tf-latest-gpu\"\n","export ZONE=\"asia-east1-c\"\n","export INSTANCE_NAME=\"your_instance_name\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"txeoBC6lGVN8","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}