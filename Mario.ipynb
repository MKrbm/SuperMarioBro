{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Mario.ipynb","provenance":[{"file_id":"1rj50M2J4iYqMorbhdAvMaLnKTaUMf7sb","timestamp":1578140931418}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"PT6nqKQkzTQ7","colab_type":"code","outputId":"2276b28f-615a-486a-b7bd-1c8a9fee055a","executionInfo":{"status":"ok","timestamp":1578147941053,"user_tz":-540,"elapsed":2044,"user":{"displayName":"keisuke murota","photoUrl":"","userId":"17224383319350112390"}},"colab":{"base_uri":"https://localhost:8080/","height":289}},"source":["!nvidia-smi"],"execution_count":78,"outputs":[{"output_type":"stream","text":["Sat Jan  4 14:25:39 2020       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 440.44       Driver Version: 418.67       CUDA Version: 10.1     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|===============================+======================+======================|\n","|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   55C    P0    55W / 149W |  11426MiB / 11441MiB |      0%      Default |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                       GPU Memory |\n","|  GPU       PID   Type   Process name                             Usage      |\n","|=============================================================================|\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"n7UXYrZA06xM","colab_type":"code","colab":{}},"source":["!apt-get install -y xvfb python-opengl > /dev/null 2>&1\n","!pip install gym pyvirtualdisplay > /dev/null 2>&1"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ts54IJPQsEYj","outputId":"cfb32975-3640-454b-b754-935c6de4fa1e","executionInfo":{"status":"ok","timestamp":1578147956363,"user_tz":-540,"elapsed":14215,"user":{"displayName":"keisuke murota","photoUrl":"","userId":"17224383319350112390"}},"colab":{"base_uri":"https://localhost:8080/","height":493}},"source":["# %%capture\n","%%bash\n","apt-get install cmake\n","apt-get install zlib1g-dev\n","pip install JSAnimation\n","pip install gym-super-mario-bros"],"execution_count":80,"outputs":[{"output_type":"stream","text":["Reading package lists...\n","Building dependency tree...\n","Reading state information...\n","cmake is already the newest version (3.10.2-1ubuntu2.18.04.1).\n","The following package was automatically installed and is no longer required:\n","  libnvidia-common-430\n","Use 'apt autoremove' to remove it.\n","0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.\n","Reading package lists...\n","Building dependency tree...\n","Reading state information...\n","zlib1g-dev is already the newest version (1:1.2.11.dfsg-0ubuntu2).\n","The following package was automatically installed and is no longer required:\n","  libnvidia-common-430\n","Use 'apt autoremove' to remove it.\n","0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.\n","Requirement already satisfied: JSAnimation in /usr/local/lib/python3.6/dist-packages (0.1)\n","Requirement already satisfied: gym-super-mario-bros in /usr/local/lib/python3.6/dist-packages (7.3.0)\n","Requirement already satisfied: nes-py>=8.0.0 in /usr/local/lib/python3.6/dist-packages (from gym-super-mario-bros) (8.1.1)\n","Requirement already satisfied: numpy>=1.12.1 in /usr/local/lib/python3.6/dist-packages (from nes-py>=8.0.0->gym-super-mario-bros) (1.17.4)\n","Requirement already satisfied: pyglet>=1.3.2 in /usr/local/lib/python3.6/dist-packages (from nes-py>=8.0.0->gym-super-mario-bros) (1.3.2)\n","Requirement already satisfied: tqdm>=4.19.5 in /usr/local/lib/python3.6/dist-packages (from nes-py>=8.0.0->gym-super-mario-bros) (4.28.1)\n","Requirement already satisfied: gym>=0.10.9 in /usr/local/lib/python3.6/dist-packages (from nes-py>=8.0.0->gym-super-mario-bros) (0.15.4)\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet>=1.3.2->nes-py>=8.0.0->gym-super-mario-bros) (0.16.0)\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (from gym>=0.10.9->nes-py>=8.0.0->gym-super-mario-bros) (4.1.2.30)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym>=0.10.9->nes-py>=8.0.0->gym-super-mario-bros) (1.3.3)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym>=0.10.9->nes-py>=8.0.0->gym-super-mario-bros) (1.12.0)\n","Requirement already satisfied: cloudpickle~=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym>=0.10.9->nes-py>=8.0.0->gym-super-mario-bros) (1.2.2)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"BhKa8jC-sQb-","colab":{}},"source":["import numpy as np\n","# import cPickle as pickle\n","import matplotlib.pyplot as plt\n","from JSAnimation.IPython_display import display_animation\n","import gym\n","from collections import deque\n","from matplotlib import animation, rc\n","from IPython.display import HTML\n","\n","%matplotlib inline"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"cUyBEmZYXvvV","colab_type":"code","outputId":"38ad342e-eaab-4aa0-9651-b68a78bb65aa","executionInfo":{"status":"ok","timestamp":1578147956559,"user_tz":-540,"elapsed":13768,"user":{"displayName":"keisuke murota","photoUrl":"","userId":"17224383319350112390"}},"colab":{"base_uri":"https://localhost:8080/","height":71}},"source":["from pyvirtualdisplay import Display\n","display = Display(visible=0, size=(400, 300))\n","display.start()"],"execution_count":82,"outputs":[{"output_type":"stream","text":["xdpyinfo was not found, X start can not be checked! Please install xdpyinfo!\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["<Display cmd_param=['Xvfb', '-br', '-nolisten', 'tcp', '-screen', '0', '400x300x24', ':1005'] cmd=['Xvfb', '-br', '-nolisten', 'tcp', '-screen', '0', '400x300x24', ':1005'] oserror=None return_code=None stdout=\"None\" stderr=\"None\" timeout_happened=False>"]},"metadata":{"tags":[]},"execution_count":82}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"3xKP6YMWulJE","colab":{}},"source":["from nes_py.wrappers import JoypadSpace\n","import gym_super_mario_bros\n","from gym_super_mario_bros.actions import SIMPLE_MOVEMENT\n","env = gym_super_mario_bros.make('SuperMarioBros-v0')\n","env = JoypadSpace(env, SIMPLE_MOVEMENT)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1578147957103,"user_tz":-540,"elapsed":14150,"user":{"displayName":"keisuke murota","photoUrl":"","userId":"17224383319350112390"}},"id":"3XMUcwKIum5U","outputId":"200efbbf-c0a6-4bfb-e449-60cb7ae01553","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["action_space = env.get_action_meanings()\n","action_space"],"execution_count":84,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['NOOP', 'right', 'right A', 'right B', 'right A B', 'A', 'left']"]},"metadata":{"tags":[]},"execution_count":84}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"VRyTL5ZzsWmx","colab":{}},"source":["def display_frames_as_gif(frames):\n","    \"\"\"\n","    Displays a list of frames as a gif, with controls\n","    \"\"\"\n","    #plt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi = 72)\n","    patch = plt.imshow(frames[0])\n","    plt.axis('off')\n","\n","    def animate(i):\n","        patch.set_data(frames[i])\n","\n","    anim = animation.FuncAnimation(plt.gcf(), animate, frames = len(frames), interval=25)\n","    return anim"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"aaRK7mLwJRGW","colab":{}},"source":["def preprocess(frame):\n","    frame = frame.sum(axis=-1)/765\n","    frame = frame[20:210,100:]\n","    frame = frame[::2,::2]\n","    return frame"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YmlvK7smUCIR","colab_type":"text"},"source":["## Models"]},{"cell_type":"code","metadata":{"id":"QWhY9hIg_w4x","colab_type":"code","colab":{}},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class QNetworkDuellingCNN1(nn.Module):\n","    def __init__(self, num_inputs, num_actions):\n","        super(ActorCritic, self).__init__()\n","        self.conv1 = nn.Conv2d(num_inputs, 32, 3, stride=2, padding=1)\n","        self.conv2 = nn.Conv2d(32, 32, 3, stride=2, padding=1)\n","        self.conv3 = nn.Conv2d(32, 32, 3, stride=2, padding=1)\n","        self.conv4 = nn.Conv2d(32, 32, 3, stride=2, padding=1)\n","        self.lstm = nn.LSTMCell(32 * 6 * 6, 512)\n","        self.critic_linear = nn.Linear(512, 1)\n","        self.actor_linear = nn.Linear(512, num_actions)\n","        self._initialize_weights()\n","\n","    def _initialize_weights(self):\n","        for module in self.modules():\n","            if isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear):\n","                nn.init.xavier_uniform_(module.weight)\n","                # nn.init.kaiming_uniform_(module.weight)\n","                nn.init.constant_(module.bias, 0)\n","            elif isinstance(module, nn.LSTMCell):\n","                nn.init.constant_(module.bias_ih, 0)\n","                nn.init.constant_(module.bias_hh, 0)\n","\n","    def forward(self, x, hx, cx):\n","        x = F.relu(self.conv1(x))\n","        x = F.relu(self.conv2(x))\n","        x = F.relu(self.conv3(x))\n","        x = F.relu(self.conv4(x))\n","        x = x.reshape(x.shape[0], -1)\n","        #hx, cx = self.lstm(x.view(x.size(0), -1), (hx, cx))\n","        return self.actor_linear(hx), self.critic_linear(hx), hx, cx\n","\n","\n","class QNetworkDuellingCNN(nn.Module):\n","    \"\"\"Actor (Policy) Model.\"\"\"\n","\n","    def __init__(self, channels, action_size, seed=42):\n","        \"\"\"Initialize parameters and build model.\n","        Params\n","        ======\n","            state_size (int): Dimension of each state\n","            action_size (int): Dimension of each action\n","            seed (int): Random seed\n","        \"\"\"\n","        super(QNetworkDuellingCNN, self).__init__()\n","        self.seed = torch.manual_seed(seed)\n","        self.conv1 = nn.Conv2d(channels, 4, 3, padding=1)\n","        self.conv2 = nn.Conv2d(4, 8, 3, padding=1)\n","        self.conv3 = nn.Conv2d(8, 16, 3, padding=1)\n","        self.conv4 = nn.Conv2d(16, 16, 3, padding=1)\n","        self.conv5 = nn.Conv2d(16, 16, 3, padding=1)\n","        self.bn1 = nn.BatchNorm2d(4)\n","        self.bn2 = nn.BatchNorm2d(8)\n","        self.bn3 = nn.BatchNorm2d(16)\n","        self.bn4 = nn.BatchNorm2d(16)\n","        self.bn5 = nn.BatchNorm2d(16)\n","        self.pool = nn.MaxPool2d(2, ceil_mode=True)\n","        \n","        flat_len = 1920#16*3*3\n","        self.fcval = nn.Linear(flat_len, 20)\n","        self.fcval2 = nn.Linear(20, 1)\n","        self.fcadv = nn.Linear(flat_len, 20)\n","        self.fcadv2 = nn.Linear(20, action_size)\n","\n","    def forward(self, x):\n","        \"\"\"Build a network that maps state -> action values.\"\"\"\n","        x = F.relu(self.bn1(self.conv1(x)))\n","        \n","        x = F.relu(self.bn2(self.conv2(x)))\n","        \n","        x = self.pool(\n","            F.relu(self.bn3(self.conv3(x))))\n","        \n","        x = self.pool(\n","            F.relu(self.bn4(self.conv4(x))))\n","        \n","        x = self.pool(\n","            F.relu(self.bn5(self.conv5(x))))\n","\n","        \n","        x = x.reshape(x.shape[0], -1)\n","        \n","        advantage = F.relu(self.fcadv(x))\n","        advantage = self.fcadv2(advantage)\n","        advantage = advantage - torch.mean(advantage, dim=-1, keepdim=True)\n","        \n","        value = F.relu(self.fcval(x))\n","        value = self.fcval2(value)\n","\n","        return value + advantage"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kizYu8ZVUWJQ","colab_type":"text"},"source":["## Memory buffer\n","Saves States, Actions, Rewards, Next States (SARS) and Dones.\n","\n","If priority sampling is required, we sample according to the error of the model."]},{"cell_type":"code","metadata":{"id":"vh4F0hv5-iHn","colab_type":"code","colab":{}},"source":["import torch\n","import numpy as np\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","class ReplayBuffer:\n","    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n","\n","    def __init__(self, state_size, action_size, buffer_size, batch_size, priority=False):\n","        \"\"\"Initialize a ReplayBuffer object.\n","        Params\n","        ======\n","            action_size (int): dimension of each action\n","            buffer_size (int): maximum size of buffer (chosen as multiple of num agents)\n","            batch_size (int): size of each training batch\n","            seed (int): random seed\n","        \"\"\"\n","        self.states = torch.zeros((buffer_size,)+state_size).to(device)\n","        self.next_states = torch.zeros((buffer_size,)+state_size).to(device)\n","        self.actions = torch.zeros(buffer_size,1, dtype=torch.long).to(device)\n","        self.rewards = torch.zeros(buffer_size, 1, dtype=torch.float).to(device)\n","        self.dones = torch.zeros(buffer_size, 1, dtype=torch.float).to(device)\n","        self.e = np.zeros((buffer_size, 1), dtype=np.float)\n","        \n","        self.priority = priority\n","\n","        self.ptr = 0\n","        self.n = 0\n","        self.buffer_size = buffer_size\n","        self.batch_size = batch_size\n","    \n","    def add(self, state, action, reward, next_state, done):\n","        \"\"\"Add a new experience to memory.\"\"\"\n","        self.states[self.ptr] = torch.from_numpy(state).to(device)\n","        self.next_states[self.ptr] = torch.from_numpy(next_state).to(device)\n","        self.actions[self.ptr] = action\n","        self.rewards[self.ptr] = reward\n","        self.dones[self.ptr] = done\n","        \n","        self.ptr += 1\n","        if self.ptr >= self.buffer_size:\n","            self.ptr = 0\n","            self.n = self.buffer_size\n","\n","    def sample(self, get_all=False):\n","        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n","        n = len(self)\n","        if get_all:\n","            return self.states[:n], self.actions[:n], self.rewards[:n], self.next_states[:n], self.dones[:n]\n","        # else:\n","        if self.priority:\n","            idx = np.random.choice(n, self.batch_size, replace=False, p=self.e)\n","        else:\n","            idx = np.random.choice(n, self.batch_size, replace=False)\n","        \n","        states = self.states[idx]\n","        next_states = self.next_states[idx]\n","        actions = self.actions[idx]\n","        rewards = self.rewards[idx]\n","        dones = self.dones[idx]\n","        \n","        return (states, actions, rewards, next_states, dones), idx\n","      \n","    def update_error(self, e, idx=None):\n","        e = torch.abs(e.detach())\n","        e = e / e.sum()\n","        if idx is not None:\n","            self.e[idx] = e.cpu().numpy()\n","        else:\n","            self.e[:len(self)] = e.cpu().numpy()\n","        \n","    def __len__(self):\n","        if self.n == 0:\n","            return self.ptr\n","        else:\n","            return self.n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6FVp2xSDU7bP","colab_type":"text"},"source":["## Agent\n","Actual agent and how to respond to the current state of the environment. Uses Models and Memory buffer from before.\n","\n","y_target = r + gamma * sum(future_rewards)\n","\n","sum(future_rewards) = q_local(state, action)"]},{"cell_type":"code","metadata":{"id":"eU94fRYi-iHq","colab_type":"code","colab":{}},"source":["import numpy as np\n","import random\n","from collections import namedtuple, deque\n","import itertools\n","\n","import torch\n","import torch.nn.functional as F\n","import torch.optim as optim\n","\n","# from ReplayBuffer import ReplayBuffer\n","\n","BUFFER_SIZE = int(5e3)  # replay buffer size\n","BATCH_SIZE = 256         # minibatch size\n","GAMMA = 0.99            # discount factor\n","TAU = 1e-3              # for soft update of target parameters\n","LR = 5e-4              # learning rate \n","UPDATE_EVERY = 10        # how often to update the network\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","            \n","\n","class DQNAgent():\n","    \"\"\"Interacts with and learns from the environment.\"\"\"\n","\n","    def __init__(self, model, state_size, action_size, seed=42, ddqn=False, priority=False):\n","        \"\"\"Initialize an Agent object.\n","        \n","        Params\n","        ======\n","            state_size (int): dimension of each state\n","            action_size (int): dimension of each action\n","            seed (int): random seed\n","        \"\"\"\n","        self.state_size = state_size\n","        self.action_size = action_size\n","        self.seed = random.seed(seed)\n","        self.ddqn = ddqn\n","\n","        # Q-Network\n","        self.qnetwork_local = model(state_size[0], action_size, seed).to(device)\n","        self.qnetwork_target = model(state_size[0], action_size, seed).to(device)\n","        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=LR)\n","\n","        # Replay memory\n","        self.memory = ReplayBuffer(state_size, (action_size,), BUFFER_SIZE, BATCH_SIZE)\n","        # Initialize time step (for updating every UPDATE_EVERY steps)\n","        self.t_step = 0\n","    \n","    def step(self, state, action, reward, next_state, done, sum_reward):\n","        # Save experience in replay memory\n","        self.memory.add(state, action, reward, next_state, done)\n","        self.sum_reward = sum_reward\n","        # Learn every UPDATE_EVERY time steps.\n","        self.t_step = (self.t_step + 1) % UPDATE_EVERY\n","        if self.t_step == 0:\n","            # If enough samples are available in memory, get random subset and learn\n","            if len(self.memory) > BATCH_SIZE:\n","                experiences, idx = self.memory.sample() # fetch all experiences correnponding to idx\n","                e = self.learn(experiences)\n","                self.memory.update_error(e, idx)\n","\n","    def act(self, state, eps=0.):\n","        \"\"\"Returns actions for given state as per current policy.\n","        \n","        Params\n","        ======\n","            state (array_like): current state\n","            eps (float): epsilon, for epsilon-greedy action selection\n","        \"\"\"\n","        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n","        self.qnetwork_local.eval() #\n","        with torch.no_grad():\n","            action_values = self.qnetwork_local(state)\n","        self.qnetwork_local.train()\n","\n","\n","        # Epsilon-greedy action selection\n","        if random.random() > eps:\n","            return np.argmax(action_values.cpu().data.numpy())\n","        else:\n","            return random.choice(np.arange(self.action_size))\n","          \n","    def update_error(self):\n","        states, actions, rewards, next_states, dones = self.memory.sample(get_all=True)\n","        with torch.no_grad():\n","            if self.ddqn:\n","                old_val = self.qnetwork_local(states).gather(-1, actions)\n","                actions = self.qnetwork_local(next_states).argmax(-1, keepdim=True)\n","                maxQ = self.qnetwork_target(next_states).gather(-1, actions)\n","                target = rewards+GAMMA*maxQ*(1-dones)\n","            else: # Normal DQN\n","                maxQ = self.qnetwork_target(next_states).max(-1, keepdim=True)[0]\n","                target = rewards+GAMMA*maxQ*(1-dones)\n","                old_val = self.qnetwork_local(states).gather(-1, actions)\n","            e = old_val - target\n","            self.memory.update_error(e)\n","\n","    def learn(self, experiences):\n","        \"\"\"Update value parameters using given batch of experience tuples.\n","        Params\n","        ======\n","            experiences (Tuple[torch.Variable]): tuple of (s, a, r, s', done) tuples \n","            gamma (float): discount factor\n","        \"\"\"\n","        states, actions, rewards, next_states, dones = experiences\n","\n","        ## compute and minimize the loss\n","        self.optimizer.zero_grad()\n","        if self.ddqn:\n","            old_val = self.qnetwork_local(states).gather(-1, actions) #Q(s,a,old_theta)\n","            with torch.no_grad():\n","                next_actions = self.qnetwork_local(next_states).argmax(-1, keepdim=True)\n","                maxQ = self.qnetwork_target(next_states).gather(-1, next_actions)\n","                target = rewards+GAMMA*maxQ*(1-dones)\n","        else: # Normal DQN\n","            with torch.no_grad():\n","                maxQ = self.qnetwork_target(next_states).max(-1, keepdim=True)[0]\n","                target = rewards+GAMMA*maxQ*(1-dones)\n","            old_val = self.qnetwork_local(states).gather(-1, actions)   \n","        \n","        loss = F.mse_loss(old_val, target) #mean square error\n","        loss.backward()\n","        self.optimizer.step()\n","\n","        # ------------------- update target network ------------------- #\n","        self.soft_update(self.qnetwork_local, self.qnetwork_target, TAU) \n","        \n","        return old_val - target\n","\n","\n","    def soft_update(self, local_model, target_model, tau):\n","        \"\"\"Soft update model parameters.\n","        θ_target = τ*θ_local + (1 - τ)*θ_target\n","        Params\n","        ======\n","            local_model (PyTorch model): weights will be copied from\n","            target_model (PyTorch model): weights will be copied to\n","            tau (float): interpolation parameter \n","        \"\"\"\n","        self.sum_reward = 400/self.sum_reward\n","        #tau *= self.sum_reward\n","        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n","            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bc8_rpzfLKLD","colab_type":"text"},"source":["## Edit Reward system\n"]},{"cell_type":"code","metadata":{"id":"FyMwjvrzLFgm","colab_type":"code","colab":{}},"source":["class make_wrapper(gym.Wrapper):\n","    def __init__(self, env=None):\n","        \"\"\"Return only every `skip`-th frame\"\"\"\n","        super(make_wrapper, self).__init__(env)\n","        self.x = 40\n","\n","    def step(self, action):\n","        total_reward = 0\n","        for _ in range(2):\n","            obs, reward, done, info = self.env.step(action) #Using the same action during for loop.\n","            total_reward += reward\n","            if done or reward < -10:\n","                break\n","        \n","        self.x_old = self.x\n","        self.x = info[\"x_pos\"]\n","        if self.x_old >= self.x:\n","            total_reward -= 0.5\n","        return obs, total_reward, done, info"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jhhfln09SeE2","colab_type":"code","colab":{}},"source":["env = gym_super_mario_bros.make('SuperMarioBros-v0')\n","env = JoypadSpace(env, SIMPLE_MOVEMENT)\n","env = make_wrapper(env)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0AVzhiInvWCC","colab_type":"code","colab":{}},"source":["net = QNetworkDuellingCNN(4,7)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_KGFLG3OVNuo","colab_type":"text"},"source":["## Train Agent\n","We step through different iterations of the environment to learn the optimal action for a given state."]},{"cell_type":"code","metadata":{"id":"2mc-TsIG-iHt","colab_type":"code","outputId":"77bbe216-3b7b-4fbd-f3a8-bb9c15828c95","executionInfo":{"status":"error","timestamp":1578147987805,"user_tz":-540,"elapsed":43668,"user":{"displayName":"keisuke murota","photoUrl":"","userId":"17224383319350112390"}},"colab":{"base_uri":"https://localhost:8080/","height":443}},"source":["episode = 100\n","discount_rate = .99\n","noise = 0.05\n","noise_decay = 0.99\n","tmax = 200\n","target_reward = 1500\n","\n","# keep track of progress\n","sum_rewards = [0]\n","\n","\n","# keep track of frames\n","FRAME_SHAPE = (95, 78) #(210-20)/2 , 256/2\n","MAX_FRAMES = 4\n","nn_frames = deque(maxlen=MAX_FRAMES)\n","for i in range(MAX_FRAMES):\n","    nn_frames.append(np.zeros(FRAME_SHAPE))\n","    \n","action_size = 7 #len(valid_actions)\n","state_size = (MAX_FRAMES,) + FRAME_SHAPE\n","agent = DQNAgent(QNetworkDuellingCNN, state_size, action_size, ddqn=True, priority=True)\n","e = 0\n","while np.mean(sum_rewards[-5:]) < target_reward:\n","    obs = env.reset()\n","    prev_obs = None\n","    sum_reward = 0\n","    \n","    for i in range(MAX_FRAMES):\n","        nn_frames.append(np.zeros(FRAME_SHAPE)) # elements of the deque are all zero\n","    nn_frames.append(np.copy(preprocess(obs)))\n","    states = np.array(nn_frames)\n","    for t in range(tmax):\n","        actions = agent.act(states, noise)\n","        obs, reward, done, _ = env.step(actions)\n","        nn_frames.append(np.copy(preprocess(obs)))\n","        next_states = np.array(nn_frames)\n","        \n","        agent.step(states, int(actions), int(reward), next_states, int(done), int(sum_rewards[-1]))\n","        sum_reward += reward\n","        states = next_states\n","        if done or reward < -10: # if mario die or game clear\n","            break\n","    \n","    agent.update_error()\n","    sum_rewards.append(sum_reward)\n","    noise = noise * noise_decay\n","    e += 1\n","    print('\\rEpisode {}\\tCurrent Score: {:.2f}'.format(e, sum_rewards[-1]), end=\"\")\n","    # display some progress every 20 iterations\n","    if (e+1) % (episode // 20) ==0:\n","        tmax = int(np.mean(sum_rewards[-5:])*1.3)\n","        print(\" | Episode: {0:d}, average score: {1:f}, tmax : {2:d}\".format(e+1,np.mean(sum_rewards[-20:]),tmax))\n","        "],"execution_count":93,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/gym_super_mario_bros/smb_env.py:148: RuntimeWarning: overflow encountered in ubyte_scalars\n","  return (self.ram[0x86] - self.ram[0x071c]) % 256\n"],"name":"stderr"},{"output_type":"stream","text":["Episode 4\tCurrent Score: 230.50 | Episode: 5, average score: 205.900000, tmax : 267\n","Episode 6\tCurrent Score: 622.00"],"name":"stdout"},{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-93-d8dd3ce4ad25>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m     \u001b[0msum_rewards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum_reward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mnoise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnoise\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnoise_decay\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-89-ab04c5ed6fac>\u001b[0m in \u001b[0;36mupdate_error\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mddqn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m                 \u001b[0mold_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqnetwork_local\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m                 \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqnetwork_local\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m                 \u001b[0mmaxQ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqnetwork_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-87-88aa9bc08000>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         x = self.pool(\n\u001b[0;32m---> 75\u001b[0;31m             F.relu(self.bn3(self.conv3(x))))\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         x = self.pool(\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrack_running_stats\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m             exponential_average_factor, self.eps)\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   1668\u001b[0m     return torch.batch_norm(\n\u001b[1;32m   1669\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1670\u001b[0;31m         \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1671\u001b[0m     )\n\u001b[1;32m   1672\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 430.00 MiB (GPU 0; 11.17 GiB total capacity; 10.03 GiB already allocated; 14.81 MiB free; 833.29 MiB cached)"]}]},{"cell_type":"code","metadata":{"id":"Xv6ptnq9zH69","colab_type":"code","colab":{}},"source":["!nvidia-smi"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"CT1aMKex-iHx","colab_type":"code","colab":{}},"source":["plt.plot(sum_rewards)\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QsYH73aq-iH3","colab_type":"code","colab":{}},"source":["obs = env.reset()\n","prev_obs = None\n","sum_reward = 0\n","\n","frames = np.zeros((2000, 240, 256, 3), dtype=np.uint8)\n","\n","for i in range(MAX_FRAMES):\n","  nn_frames.append(np.zeros(FRAME_SHAPE))\n","nn_frames.append(np.copy(preprocess(obs)))\n","states = np.array(nn_frames)\n","rs = []\n","xs = []\n","ys = []\n","i=0\n","for t in range(2000):\n","    i += 1\n","    frames[t] = obs\n","    actions = agent.act(states, noise)\n","    obs, reward, done, info = env.step(actions)\n","    nn_frames.append(np.copy(preprocess(obs)))\n","    next_states = np.array(nn_frames)\n","\n","    sum_reward += reward\n","    states = next_states\n","    rs.append(reward)\n","    xs.append(info['x_pos'])\n","    ys.append(info['y_pos'])\n","    if done:\n","        print(\"i=\",i)\n","        break\n","\n","print('Sum of rewards is ', sum(rs))\n","plt.plot(rs)\n","plt.show()\n","\n","plt.plot(xs)\n","plt.show()\n","\n","anim=display_frames_as_gif(frames)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"xkRbsYRw3IHP","colab_type":"code","colab":{}},"source":["print(np.shape(rs),i)\n","print(done)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ewYFgexmT96f","colab_type":"code","colab":{}},"source":["HTML(anim.to_html5_video())\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZZTJsONqQD_8","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"YobFvUL1pITi","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"57eed4be-1fb1-4f15-e728-e61fbac594de","executionInfo":{"status":"ok","timestamp":1578148024465,"user_tz":-540,"elapsed":725,"user":{"displayName":"keisuke murota","photoUrl":"","userId":"17224383319350112390"}}},"source":["# Mount Google Drive\n","from google.colab import drive # import drive from google colab\n","\n","ROOT = \"/content/drive\"     # default location for the drive\n","print(ROOT)                 # print content of ROOT (Optional)\n","\n","drive.mount(ROOT)           # we mount the google drive at /content/drive"],"execution_count":94,"outputs":[{"output_type":"stream","text":["/content/drive\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","outputId":"5ffe0cfe-c5c4-4392-f200-e88a0b2d2112","executionInfo":{"status":"ok","timestamp":1578146243189,"user_tz":-540,"elapsed":2075,"user":{"displayName":"keisuke murota","photoUrl":"","userId":"17224383319350112390"}},"id":"166fMASfpITp","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["!pwd"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/content\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","outputId":"a26166c3-0dc0-43c9-960d-08b992d21026","executionInfo":{"status":"ok","timestamp":1578146243191,"user_tz":-540,"elapsed":400,"user":{"displayName":"keisuke murota","photoUrl":"","userId":"17224383319350112390"}},"id":"kAaos4QPpITt","colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["# Clone github repository setup\n","# import join used to join ROOT path and MY_GOOGLE_DRIVE_PATH\n","from os.path import join  \n","\n","# path to your project on Google Drive\n","MY_GOOGLE_DRIVE_PATH = \"drive/My Drive/supermario/\"\n","# replace with your Github username \n","GIT_USERNAME = \"keisukemurota\" \n","# definitely replace with your\n","GIT_TOKEN = \"192b1acaea94387f7c0eee29af4f10ac97c2f26c\"  \n","# Replace with your github repository in this case we want \n","# to clone deep-learning-v2-pytorch repository\n","GIT_REPOSITORY = \"SuperMarioBro\" \n","\n","PROJECT_PATH = join(ROOT, MY_GOOGLE_DRIVE_PATH)\n","\n","# It's good to print out the value if you are not sure \n","print(\"PROJECT_PATH: \", PROJECT_PATH)   \n","\n","# In case we haven't created the folder already; we will create a folder in the project path \n","#!mkdir \"{PROJECT_PATH}\"    \n","\n","#GIT_PATH = \"https://{GIT_TOKEN}@github.com/{GIT_USERNAME}/{GIT_REPOSITORY}.git\" this return 400 Bad Request for me\n","GIT_PATH = \"https://\" + GIT_TOKEN + \"@github.com/\" + GIT_USERNAME + \"/\" + GIT_REPOSITORY + \".git\"\n","print(\"GIT_PATH: \", GIT_PATH)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["PROJECT_PATH:  /content/drive/drive/My Drive/supermario/\n","GIT_PATH:  https://192b1acaea94387f7c0eee29af4f10ac97c2f26c@github.com/keisukemurota/SuperMarioBro.git\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","outputId":"97f16bb3-daeb-4aed-c906-de84fc6b64c0","executionInfo":{"status":"ok","timestamp":1578148034661,"user_tz":-540,"elapsed":450,"user":{"displayName":"keisuke murota","photoUrl":"","userId":"17224383319350112390"}},"id":"McTdae-WpITv","colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["path = \"drive/My Drive/supermario/SuperMarioBro\"\n","%cd \"{path}\""],"execution_count":95,"outputs":[{"output_type":"stream","text":["[Errno 2] No such file or directory: 'drive/My Drive/supermario/SuperMarioBro'\n","/content/drive/My Drive/supermario/SuperMarioBro\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"IY16Xte_pITx","colab":{}},"source":["!git add .\n","!git commit -m \"changed\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"9HtlLLOppITz","colab":{}},"source":["!git config --global user.email \"mukeisuke0709@gmail.com\"\n","!git config --global user.name \"keisukemurota\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"FW_iUUNnJBkn","colab_type":"code","outputId":"ef234869-97d9-46b1-bc31-079a8d47789c","executionInfo":{"status":"ok","timestamp":1578117650971,"user_tz":-540,"elapsed":481,"user":{"displayName":"keisuke murota","photoUrl":"","userId":"17224383319350112390"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["%cd SuperMarioBro/"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/content/SuperMarioBro\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"gHEL8E8GJFfu","colab_type":"code","outputId":"d677a621-ce21-4c94-b838-36e5b516366c","executionInfo":{"status":"ok","timestamp":1578117664301,"user_tz":-540,"elapsed":2097,"user":{"displayName":"keisuke murota","photoUrl":"","userId":"17224383319350112390"}},"colab":{"base_uri":"https://localhost:8080/","height":85}},"source":["! git status"],"execution_count":0,"outputs":[{"output_type":"stream","text":["On branch master\n","Your branch is up to date with 'origin/master'.\n","\n","nothing to commit, working tree clean\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"9fRDPSYvJIW5","colab_type":"code","outputId":"5573c367-57e9-45e1-f8bd-7f1bcec1a432","executionInfo":{"status":"ok","timestamp":1578117688185,"user_tz":-540,"elapsed":2196,"user":{"displayName":"keisuke murota","photoUrl":"","userId":"17224383319350112390"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["! ls"],"execution_count":0,"outputs":[{"output_type":"stream","text":["'hello world.ipynb'   README.md\t\t        test.py\n","'README copy.md'      SuperDeepMarioBro.ipynb\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"29Pj_AFHJNKO","colab_type":"code","outputId":"a953929d-8486-41b9-c5ca-990f10feaf30","executionInfo":{"status":"ok","timestamp":1578143296281,"user_tz":-540,"elapsed":2730,"user":{"displayName":"keisuke murota","photoUrl":"","userId":"17224383319350112390"}},"colab":{"base_uri":"https://localhost:8080/","height":85}},"source":["!git clone https://github.com/davidchiou/DDQN_Mario.git"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Cloning into 'DDQN_Mario'...\n","remote: Enumerating objects: 44, done.\u001b[K\n","remote: Total 44 (delta 0), reused 0 (delta 0), pack-reused 44\u001b[K\n","Unpacking objects: 100% (44/44), done.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"-mKFNzY0qsld","colab_type":"code","outputId":"f8a4158e-2c60-487b-8094-c815510b1a61","executionInfo":{"status":"ok","timestamp":1578143441971,"user_tz":-540,"elapsed":2214,"user":{"displayName":"keisuke murota","photoUrl":"","userId":"17224383319350112390"}},"colab":{"base_uri":"https://localhost:8080/","height":102}},"source":["!python main.py"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Saved to statistics.pkl\n","Traceback (most recent call last):\n","  File \"main.py\", line 8, in <module>\n","    import ppaquette_gym_super_mario\n","ModuleNotFoundError: No module named 'ppaquette_gym_super_mario'\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"HAbMm5Lxq7YP","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}